# -*- coding: utf-8 -*-
"""GPU Program.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L2_YgNDjdQciBH1_nN0gd_KTxhrCYpOa
"""

!pip install tensorflow-gpu

!pip install tensorflow-gpu

import tensorflow as tf
print("TensorFlow version:", tf.__version__)
print("GPU available:", tf.config.list_physical_devices('GPU'))

import tensorflow as tf
print("TensorFlow version:", tf.__version__)
print("GPU available:", tf.config.list_physical_devices('GPU'))

pip install matplotlib
import tensorflow as tf

from tensorflow.keras import layers, models

import numpy as np
import matplotlib.pyplot as plt


def create_model():
    model = models.Sequential([


        layers.Dense(128, activation='relu', input_shape=(784,)),
        layers.Dropout(0.2),

        layers.Dense(10, activation='softmax')

    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Load MNIST dataset
mnist = tf.keras.datasets.mnist
print("Loaded MNIST dataset")

# Load and preprocess the data
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0 #normalizsing with the pixel values from 0 to 255

# View some examples from the dataset
#printing dataset for visualization
print("Example images from the MNIST dataset:")
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i], cmap=plt.cm.binary)
    plt.xlabel(y_train[i])  # Show the corresponding label
plt.show()

# Flatten the images
x_train_flattened = x_train.reshape(-1, 784)
x_test_flattened = x_test.reshape(-1, 784)

# Create and compile the model
model = create_model()

# Print model summary
model.summary()

# Train the model
history = model.fit(x_train_flattened, y_train, epochs=5, validation_data=(x_test_flattened, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(x_test_flattened, y_test)
print('\nTest accuracy:', test_acc)
!nvidia-smi

pip install matplotlib

import tensorflow as tf

from tensorflow.keras import layers, models

import numpy as np
import matplotlib.pyplot as plt


def create_model():
    model = models.Sequential([


        layers.Dense(128, activation='relu', input_shape=(784,)),
        layers.Dropout(0.2),

        layers.Dense(10, activation='softmax')

    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Load MNIST dataset
mnist = tf.keras.datasets.mnist
print("Loaded MNIST dataset")

# Load and preprocess the data
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0 #normalizsing with the pixel values from 0 to 255

# View some examples from the dataset
#printing dataset for visualization
print("Example images from the MNIST dataset:")
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i], cmap=plt.cm.binary)
    plt.xlabel(y_train[i])  # Show the corresponding label
plt.show()

# Flatten the images
x_train_flattened = x_train.reshape(-1, 784)
x_test_flattened = x_test.reshape(-1, 784)

# Create and compile the model
model = create_model()

# Print model summary
model.summary()

# Train the model
history = model.fit(x_train_flattened, y_train, epochs=5, validation_data=(x_test_flattened, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(x_test_flattened, y_test)
print('\nTest accuracy:', test_acc)

!nvidia-smi

!pip install pycuda

import pycuda.autoinit
import pycuda.driver as cuda
print("PyCUDA installed and initialized successfully!")

import tensorflow as tf

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

import tensorflow as tf
import pycuda.autoinit
import pycuda.driver as cuda
from pycuda.compiler import SourceModule
import numpy as np


kernel_code = """
__global__ void normalize(float *input, float *output, int size) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx < size) {
        output[idx] = input[idx] / 255.0;  // Normalize pixel values to [0, 1]
    }
}
"""

# Compile the CUDA kernel
mod = SourceModule(kernel_code)
normalize_kernel = mod.get_function("normalize")

# Custom CUDA function for normalizing MNIST images
def cuda_normalize(images):
    # Flatten the input images and convert to float32
    images = images.astype(np.float32).flatten()
    size = images.size

    # Allocate memory on the GPU
    input_gpu = cuda.mem_alloc(images.nbytes)
    output_gpu = cuda.mem_alloc(images.nbytes)

    # Copy input data to the GPU
    cuda.memcpy_htod(input_gpu, images)

    # Allocate space for the result
    normalized_images = np.empty_like(images)

    # Define block and grid sizes
    block_size = 256
    grid_size = (size + block_size - 1) // block_size

    # Launch the kernel
    normalize_kernel(input_gpu, cuda.Out(normalized_images), np.int32(size), block=(block_size, 1, 1), grid=(grid_size, 1, 1))

    # Reshape the normalized images back to their original shape
    return normalized_images.reshape(-1, 28, 28)

# Example usage with MNIST dataset
if __name__ == "__main__":
    # Load MNIST dataset
    mnist = tf.keras.datasets.mnist
    (x_train, _), (_, _) = mnist.load_data()

    # Normalize using CUDA
    normalized_data = cuda_normalize(x_train)

    print("Original Data (First Image):\n", x_train[0])
    print("\nNormalized Data (First Image):\n", normalized_data[0])